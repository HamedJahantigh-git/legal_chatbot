{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<div style=\"text-align: right\">\n",
    "<h1>استخراج ویژگی‌ها از متن</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".right-align-list {\n",
    "  direction: rtl; \n",
    "  list-style-position: inside; \n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    ".right-align-list li {\n",
    "  text-align: right;\n",
    "  padding-right: 20px;\n",
    "}\n",
    "</style>\n",
    "<div style=\"text-align: right\">\n",
    "به دلیل تنوع بالای متون حقوقی، ویژگی‌های قابل استخراج از این متون نیز متنوع می‌باشد. در اینجا ما به استخراج 4 ویژگی مهم این متون پرداختیم که عبارتند از\n",
    "</div>\n",
    "<ul class=\"right-align-list\">\n",
    "  <li>استخراج ماده و اصول</li>\n",
    "  <li>استخراج قوانین</li>\n",
    "  <li>استخراج تاریخ‌ها</li>\n",
    "  <li>استخراج ؟</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<h2>استخراج ماده و اصول</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".right-align-list {\n",
    "  direction: rtl; \n",
    "  list-style-position: inside; \n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    ".right-align-list li {\n",
    "  text-align: right;\n",
    "  padding-right: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "جهت استخراج مواد چند کلمه کلیدی {'ماده', 'مواد', 'اصل', 'اصول'} را به عنوان شاخص جستجو در نظر گرفتیم.\n",
    "متن را به جملات کوتاه شکسته و هر جمله را با کمک کتابخانه هضم چانک بندی میکنیم. از میان چانک‌ها، آن‌هایی که برچسب‌های زیر را دارند جدا میکنیم\n",
    "\n",
    "<ul class=\"right-align-list\">\n",
    "  <li>گروه اسمی - NP</li>\n",
    "  <li> گروه قیدی - ADVP</li>\n",
    "  <li>گروه وصفی - ADJP </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: right; direction: rtl;\">\n",
    "    پس از آن با کمک کلاس <code>find_article</code> به دنبال این کلمات کلیدی می‌گردیم.\n",
    "    دو حالت را در نظر میگیریم:\n",
    "    <br>\n",
    "    حالت اول: ماده و قانون در یک چانک باشند\n",
    "    <br>\n",
    "    حالت دوم: چانک فاقد قانون بوده و فقط دارای ماده باشد.\n",
    "    <br>\n",
    "    در هر حالت پس از یافتن ماده و نقطه شروع و پایان، آن را به تابع اصلی بازگردانده و در آخر لیستی شامل مواد و span را باز میگردانیم\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<h2>استخراج قوانین</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".right-align-list {\n",
    "  direction: rtl; \n",
    "  list-style-position: inside; \n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    ".right-align-list li {\n",
    "  text-align: right;\n",
    "  padding-right: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"text-align: right;  direction: rtl;\">\n",
    "جهت استخراج قوانین نیز مجموعه ایی از کلمات کلیدی را در نظر گرفتیم که عبارتند از: \n",
    "<br>\n",
    " ['قانون', 'قوانین', 'آیین نامه', 'آیین‌نامه', 'اساس نامه', 'اساس‌نامه']\n",
    "<br>\n",
    "پس از آن بررسی میشود که کدامیک از این موارد در POS وجود دارد.\n",
    "<br>\n",
    "پس از این گام توسط الگوریتمی که به دنبال حرف اضافه EZ میگردد، قوانین را استخراج میکند. بدین صورت که افزودن واژگان را تا جایی که EZ هست ادامه می‌دهد، کلمه بعد از EZ را نیز اضافه میکند. در صورتی که ویرگول یا \"و\" وجود داشته باشد، آن را نیز استخراج و اضافه میکند.<br>\n",
    "لازم به ذکر است که هدف ابتدایی ما برای یافتن قوانین به شرح زیر بود:\n",
    "<br>\n",
    "ما لیستی از قوانین را استخراج نمودیم و سپس توسط bleu score و bert score به دنبال نزدیک ترین قانون به قانون متن گشتیم. اما به دلیل نقص لیست قوانین نتایج استخراج شده مطلوب نبوده و ما را از ادامه این مسیر منصرف نمود.\n",
    "</div>\n",
    "\n",
=======
    "from hazm import *\n",
    "import sacrebleu\n",
    "\n",
    "class law_extractor(object):\n",
    "\n",
    "    listOfLaw = []\n",
    "\n",
    "\n",
    "    with open('resource/law/law_clean_list.txt', 'r', encoding='utf-8') as file:\n",
    "        laws = file.readlines()\n",
    "\n",
    "    for law in laws:\n",
    "        listOfLaw.append(law)\n",
    "\n",
    "    keywords = [\n",
    "        'قانون' , 'قوانین' , 'آیین نامه'  , 'آیین‌نامه' , 'اساس نامه' , 'اساس‌نامه'\n",
    "    ]\n",
    "\n",
    "    def senTokenizer(self, text):\n",
    "\n",
    "        tagger = POSTagger(model='resource/hazm_model/pos_tagger.model')\n",
    "        chunker = Chunker(model='resource/hazm_model/chunker.model')\n",
    "\n",
    "        chunks = []\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            words = word_tokenize(sentence)\n",
    "            tagged_words = tagger.tag(words)\n",
    "            print(tagged_words)\n",
    "\n",
    "            tree = chunker.parse(tagged_words)\n",
    "\n",
    "            for subtree in tree.subtrees():\n",
    "                # if subtree.label() in ['NP', 'ADVP']:\n",
    "                chunk_text = tree2brackets(subtree)\n",
    "\n",
    "                chunks.append(chunk_text)\n",
    "        # print(chunks)\n",
    "        return chunks\n",
    "\n",
    "    def find_law(self, tokens):\n",
    "        dates = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            chunk = tokens[i]\n",
    "\n",
    "            if any(key in chunk for key in self.keywords):\n",
    "                dates.append(chunk)\n",
    "\n",
    "            i += 1  \n",
    "        return dates\n",
    "\n",
    "    def check_law(self, law):\n",
    "        splitedLaw = law.split()\n",
    "        if splitedLaw[-1] in self.keywords:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def selected_law(self, law):\n",
    "        splitedLaw = law.split()\n",
    "        for splited in splitedLaw:\n",
    "            if splited in self.keywords:\n",
    "                index = splitedLaw.index(splited)\n",
    "        \n",
    "        return ' '.join(splitedLaw[index:])\n",
    "\n",
    "    def bleu_score(self, candidate, references):\n",
    "\n",
    "        scores = []\n",
    "        \n",
    "        for ref in references:\n",
    "            bleu = sacrebleu.corpus_bleu([candidate], [[ref]])\n",
    "            scores.append(bleu.score)\n",
    "        \n",
    "\n",
    "        max_index = scores.index(max(scores))\n",
    "        return max_index, scores[max_index] , scores\n",
    "\n",
    "    def find_in_text(self, text , law):\n",
    "\n",
    "        for key in self.keywords:\n",
    "            if key in law:\n",
    "                findKey = key\n",
    "                break\n",
    "        \n",
    "        start_index = text.find(findKey)\n",
    "        if start_index == -1:\n",
    "            return -1, -1 \n",
    "        \n",
    "        end_index = start_index + len(law) - 1\n",
    "\n",
    "        return start_index, end_index\n",
    "\n",
    "    def __init__(self, text):\n",
    "        returnList = []\n",
    "        tokenized_sentences = self.senTokenizer(text)\n",
    "        laws = self.find_law(tokenized_sentences)\n",
    "\n",
    "        for law in laws:\n",
    "            if self.check_law(law):\n",
    "                law = self.selected_law(law)\n",
    "                myList = []\n",
    "                start , end = self.find_in_text(text , law)\n",
    "                myList.append((law, start, end))\n",
    "                returnList.append(myList)\n",
    "                ind, score , scores = self.bleu_score(law , self.listOfLaw)\n",
    "                l = self.listOfLaw[ind]\n",
    "                print(l)\n",
    "                print(ind , score)\n",
    "        self.result = returnList\n",
    "\n",
    "text = \"هیئت وزیران در جلسه ۱۱/۲/۱۴۰۱ به پیشنهاد مشترک سازمان‌های برنامه و بودجه کشور و اداری و استخدامی کشور و به استناد اصل یکصد و سی و هشتم قانون اساسی جمهوری اسلامی ایران، ماده (۷۶) قانون مدیریت خدمات کشوری -مصوب ۱۳۸۶- و بند (پ) ماده (۲۸) قانون برنامه ششم توسعه اقتصادی، اجتماعی و فرهنگی جمهوری اسلامی ایران -مصوب ۱۳۹۵- و جزءهای (۱)، (۲)، (۴)، (۶)، (۸) و (۱۲) بند (الف) و بند (و) تبصره (۱۲) و بند (ی) تبصره (۱۹) ماده واحده قانون بودجه سال ۱۴۰۱ کل کشور تصویب کرد\"\n",
    "extractor = law_extractor(text)\n",
    "print(extractor.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"هیئت وزیران در جلسه ۱۱/۲/۱۴۰۱ به پیشنهاد مشترک سازمان‌های برنامه و بودجه کشور و اداری و استخدامی کشور و به استناد اصل یکصد و سی و هشتم قانون اساسی جمهوری اسلامی ایران، ماده (۷۶) قانون مدیریت خدمات کشوری -مصوب ۱۳۸۶- و بند (پ) ماده (۲۸) قانون برنامه ششم توسعه اقتصادی، اجتماعی و فرهنگی جمهوری اسلامی ایران -مصوب ۱۳۹۵- و جزءهای (۱)، (۲)، (۴)، (۶)، (۸) و (۱۲) بند (الف) و بند (و) تبصره (۱۲) و بند (ی) تبصره (۱۹) ماده واحده قانون بودجه سال ۱۴۰۱ کل کشور تصویب کرد\"\n",
    "from hazm import *\n",
    "tagger = POSTagger(model='resource/hazm_model/pos_tagger.model')\n",
    "chunker = Chunker(model='resource/hazm_model/chunker.model')\n",
>>>>>>> 8dfb370760e528d270654fff3b2fa35f2329b11b
    "\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<h2>استخراج تاریخ</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".right-align-list {\n",
    "  direction: rtl; \n",
    "  list-style-position: inside; \n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    ".right-align-list li {\n",
    "  text-align: right;\n",
    "  padding-right: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div style=\"text-align: right;  direction: rtl;\">\n",
    "جهت استخراج تاریخ دو گام را طی نمودیم چرا که با دو نوع متنی و عددی سر و کار داشتیم:\n",
    "<br>\n",
    "1: با کمک regex فرمت‌های عددی تاریخ را از متن استخراج نمودیم\n",
    "<br>\n",
    "2: با کمک کلمات کلیدی و چانک‌ها فرمت‌های متنی تاریخ را استخراج نمودیم\n",
    "<br>\n",
    "پس از یافتن تاریخ، همچون دو ویژگی دیگر، هر تاریخ را همراه با span بازگرداندیم.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
=======
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هیئت وزیران در جلسه ۱۱/۲/۱۴۰۱ به پیشنهاد مشترک سازمان\\u200cهای برنامه و بودجه کشور و اداری و استخدامی کشور و به استناد اصل یکصد و سی و هشتم قانون اساسی جمهوری اسلامی ایران، ماده (۷۶) قانون مدیریت خدمات کشوری -مصوب ۱۳۸۶- و بند (پ) ماده (۲۸) قانون برنامه ششم توسعه اقتصادی، اجتماعی و فرهنگی جمهوری اسلامی ایران -مصوب ۱۳۹۵- و جزءهای (۱)، (۲)، (۴)، (۶)، (۸) و (۱۲) بند (الف) و بند (و) تبصره (۱۲) و بند (ی) تبصره (۱۹) ماده واحده قانون بودجه سال ۱۴۰۱ کل کشور تصویب کرد']\n"
     ]
    }
   ],
   "source": [
    "sent_toks = sent_tokenize(text)\n",
    "print(sent_toks)\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag = tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chaunk_text = chunker.parse(taged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from hazm import Normalizer, word_tokenize\n",
    "\n",
    "class LawExtractor():\n",
    "    \n",
    "    def __init__(\n",
    "        self, normalizer = Normalizer,\n",
    "        tokenizer = word_tokenize, \n",
    "        keywords = [\n",
    "        'قانون' , 'قوانین' , 'آیین نامه'  , 'آیین‌نامه' ,\n",
    "        'اساس نامه' , 'اساس‌نامه']) -> None:\n",
    "        self._tokenizer = tokenizer\n",
    "        self._normalizer = Normalizer()\n",
    "        self._keywords = keywords\n",
    "        \n",
    "    def extract(self, input: str) -> List[Tuple[str, int, int]]:\n",
    "        pass\n",
    "    \n",
    "    def pos_analysis(self, pos_list: List) -> List:\n",
    "        end_of_list = len(pos_list)\n",
    "        index = 0\n",
    "        while index<end_of_list:\n",
    "            word = pos_list[index][0]\n",
    "            tag = pos_list[index][1]\n",
    "            if word in self._keywords and \"EZ\" in tag:\n",
    "                accept_phrase = word\n",
    "                while ez\n",
    "                    word = pos_list[index][0]\n",
    "                    tag = pos_list[index][1]\n",
    "    def _concat_correct_phrase(\n",
    "        self,\n",
    "        corrent_word: str,\n",
    "        next_word: str,\n",
    "        two_next_word: str) -> int:\n",
    "        index = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هیئت \n",
      " NOUN,EZ\n"
     ]
    }
   ],
   "source": [
    "law_extractor = LawExtractor()\n",
    "law_extractor.pos_analysis(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 8dfb370760e528d270654fff3b2fa35f2329b11b
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
