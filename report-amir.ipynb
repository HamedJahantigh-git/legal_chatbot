{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\">\n",
    "    <h1>  گزارش بخش org_extractor:</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\">\n",
    " هدف از انجام این تسک، استخراج نام تمامی سازمان های موجود در متن ورودی است. به همین جهت مجموعه ی نسبتا مفصلی از نام سازمان های کشور تهیه گردید. \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "در ابتدا، برای استفاده از منابعی مانند مجموعه ی نام سازمان ها و مدل های هضم به روش زیر، منابع لازم را فراهم می کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid model file 'resource/hazm_model/postagger.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(DICT_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     10\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer()\n\u001b[0;32m---> 11\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPOSTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTAGGER_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m chunker \u001b[38;5;241m=\u001b[39m Chunker(model\u001b[38;5;241m=\u001b[39mCHUNKER_PATH)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/hazm/pos_tagger.py:40\u001b[0m, in \u001b[0;36mPOSTagger.__init__\u001b[0;34m(self, model, data_maker, universal_tag)\u001b[0m\n\u001b[1;32m     38\u001b[0m data_maker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_maker \u001b[38;5;28;01mif\u001b[39;00m data_maker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data_maker\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_universal \u001b[38;5;241m=\u001b[39m universal_tag\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_maker\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/hazm/sequence_tagger.py:67\u001b[0m, in \u001b[0;36mSequenceTagger.__init__\u001b[0;34m(self, model, data_maker)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenceTagger\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data_maker\u001b[38;5;241m=\u001b[39mdata_maker) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/hazm/sequence_tagger.py:113\u001b[0m, in \u001b[0;36mSequenceTagger.load_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"فایل تگر را بارگزاری می‌کند.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mExamples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m tagger \u001b[38;5;241m=\u001b[39m Tagger()\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m tagger\n",
      "File \u001b[0;32mpycrfsuite/_pycrfsuite.pyx:571\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpycrfsuite/_pycrfsuite.pyx:733\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger._check_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpycrfsuite/_pycrfsuite.pyx:738\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.Tagger._check_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid model file 'resource/hazm_model/postagger.model'"
     ]
    }
   ],
   "source": [
    "from hazm import * \n",
    "import re\n",
    "\n",
    "DICT_PATH = 'resource/org/orgs.txt'\n",
    "TAGGER_PATH = 'resource/hazm_model/postagger.model'\n",
    "CHUNKER_PATH = 'resource/hazm_model/chunker.model'\n",
    "ROLES = ['NP', 'PP', 'VP', 'ADJP', 'ADVP']\n",
    "\n",
    "content = open(DICT_PATH, 'r+').readlines()\n",
    "normalizer = Normalizer()\n",
    "tagger = POSTagger(model=TAGGER_PATH)\n",
    "chunker = Chunker(model=CHUNKER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "جهت ساختاربندی مناسب، از کلاس org_extractor استفاده می کنیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class org_extractor:\n",
    "    \n",
    "    def __init__(self, text) -> None:\n",
    "        self.text = text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "در اولین گام راه حل، به کمک\n",
    "chunker\n",
    "کتابخانه ی هضم متن را چانک بندی می کنیم.\n",
    "سپس کاراکترهای زائد متن که به حل مسئله کمکی نمی کنند را حذف کرده و چانک ها را برای مرحله ی بعد آماده می کنیم.<br/>\n",
    "* بخش آخر به دلیل مدیریت کردن اشتباهات در چانک ها پس از تبدیل به فرم مورد نیاز اضافه شده\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mnormalizer\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m      2\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger\u001b[38;5;241m.\u001b[39mtag(word_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m      3\u001b[0m chunks \u001b[38;5;241m=\u001b[39m tree2brackets(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunker\u001b[38;5;241m.\u001b[39mparse(tagged))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "    def chunk_text(self):\n",
    "        self.text = self.normalizer.normalize(self.text)\n",
    "        tagged = self.tagger.tag(word_tokenize(self.text))\n",
    "        chunks = tree2brackets(self.chunker.parse(tagged))\n",
    "\n",
    "        chunked = [[c.strip(), c.split()[-1]] for c in\\\n",
    "                    [re.sub(r'\\.|،|]*|,|;|/|[۰-۹]+|\\)|\\(|-|«|»', '', chunk).strip() for chunk in chunks.split('[')] if len(c.split()) >= 2]\n",
    "\n",
    "        for c in chunked:\n",
    "            c[0] = c[0].replace('\\u200c', ' ')\n",
    "\n",
    "        roles_set = set(self.ROLES)\n",
    "\n",
    "        for chunk in chunked:\n",
    "            words = chunk[0].split()\n",
    "            filtered_words = [word for word in words if word not in roles_set]\n",
    "\n",
    "            chunk[0] = ' '.join(filtered_words)\n",
    "\n",
    "            if chunk[1] not in roles_set:\n",
    "                for word in words:\n",
    "                    if word in roles_set:\n",
    "                        chunk[1] = word \n",
    "                        break  \n",
    "\n",
    "        return chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "می دانیم نام سازمان ها در یک چانک NP قرار می گیرد. اما گاهی این نام به دلیل طولانی بودن شکسته شده و در چند چانک متوالی NP جایگیری می کند. به همین دلیل تمامی چانک های NP متوالی را ادغام میکنیم و صرفا آنها را در نظر میگیریم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def merge_np_chunks(self, chunked):\n",
    "        nps, temp, id = [], '', 0\n",
    "\n",
    "        while id < len(chunked):\n",
    "            chunk = chunked[id]\n",
    "            if chunk[1]=='NP':\n",
    "                temp = chunk[0]\n",
    "                while id<len(chunked)-1 and chunked[id+1][1]=='NP':\n",
    "                    temp = temp + ' ' + chunked[id+1][0]\n",
    "                    id = id + 1\n",
    "                    \n",
    "                else:\n",
    "                    nps.append(temp)\n",
    "                    temp = ''\n",
    "            id= id+1\n",
    "\n",
    "        return nps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "در ادامه برای هر چانک از تابع ngram استفاده می کنیم. مقدار n  در هر یک از این ngramها به ترتیب از تعداد کلمات هر چانک به یک کلمه کاهش می یابند.<br/>\n",
    "دلیل این کار آن است که به ترتیب از بزرگترین عبارت در مجموعه ی نام سازمان ها جستجو شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(self, nps):\n",
    "    ngs = []\n",
    "    for np in nps:\n",
    "        npg = []\n",
    "        count = len(np.split())\n",
    "        for i in range(count, 0, -1):\n",
    "            npg.extend(self.ngrams(np, i))\n",
    "        ngs.append(npg)\n",
    "\n",
    "    return ngs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;\"> \n",
    "سپس به کمک عبارات قاعده محور،‌ ngramهای ایجاد شده را در مجموعه ی نام سازمانها جستجو میکنیم.\n",
    "خروجی لیستی از تاپل هاست که در هر تاپل نام سازمان و اندیس شروع و پایان آن در جمله قرار دارد. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_org(self):\n",
    "\n",
    "        chunked = self.chunk_text()\n",
    "        nps = self.merge_np_chunks(chunked=chunked)\n",
    "        ngs = self.make_ngrams(nps=nps)\n",
    "        \n",
    "        output = []\n",
    "        for row in ngs:\n",
    "            t = []\n",
    "            for ng in row: \n",
    "                if len(ng) < 4:\n",
    "                    continue\n",
    "                for org in self.content:\n",
    "                    otp = re.match(f'^{ng}.*', org)\n",
    "\n",
    "                    if isinstance(otp, re.Match):\n",
    "                        t.append((otp))\n",
    "            \n",
    "            output.append(t)\n",
    "\n",
    "        result = []\n",
    "        for ng, otp in zip(ngs, output):\n",
    "            for o in otp:\n",
    "                mtch = o.group(0)\n",
    "                if mtch in ng and mtch not in result:\n",
    "                    result.append(mtch)\n",
    "        \n",
    "        result = sorted(result, key=len, reverse=True)\n",
    "        defined_terms = []\n",
    "\n",
    "        for term in result:\n",
    "            if not any(term in other for other in defined_terms):\n",
    "                for m in re.finditer(term, self.text):\n",
    "                    defined_terms.append((term, (m.start(), m.end())))\n",
    "        \n",
    "        return defined_terms\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
